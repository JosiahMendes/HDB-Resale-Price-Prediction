{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Resale-Flat-Prices.csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import math as math\n",
    "df = pd.read_csv(\"data/resale-flat-prices-sers-removed.csv\", low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine Multi-Generation and Multi Generation categories in flat type\n",
    "# Report Section 3-B-1\n",
    "df[\"flat_type\"] = df[\"flat_type\"].replace(\"MULTI GENERATION\", \"MULTI-GENERATION\")\n",
    "df[\"flat_type\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean flat model column by capitalising and renaming\n",
    "# Report Section 3-B-2\n",
    "df[\"flat_model\"] = df[\"flat_model\"].replace(\"2-room\", \"2-ROOM\")\n",
    "df[\"flat_model\"] = df[\"flat_model\"].replace(\"2-ROOM\", \"2 ROOM\")\n",
    "df[\"flat_model\"] = df[\"flat_model\"].replace(\"3Gen\", \"3 GEN\")\n",
    "df[\"flat_model\"] = df[\"flat_model\"].replace(\"Adjoined flat\", \"ADJOINED FLAT\")\n",
    "df[\"flat_model\"] = df[\"flat_model\"].replace(\"Apartment\", \"APARTMENT\")\n",
    "df[\"flat_model\"] = df[\"flat_model\"].replace(\"Improved\", \"IMPROVED\")\n",
    "df[\"flat_model\"] = df[\"flat_model\"].replace(\"Improved-Maisonette\", \"IMPROVED-MAISONETTE\")\n",
    "df[\"flat_model\"] = df[\"flat_model\"].replace(\"IMPROVED-MAISONETTE\", \"IMPROVED MAISONETTE\")\n",
    "df[\"flat_model\"] = df[\"flat_model\"].replace(\"Maisonette\", \"MAISONETTE\")\n",
    "df[\"flat_model\"] = df[\"flat_model\"].replace(\"Model A\", \"MODEL A\")\n",
    "df[\"flat_model\"] = df[\"flat_model\"].replace(\"Model A-Maisonette\", \"MODEL A-MAISONETTE\")\n",
    "df[\"flat_model\"] = df[\"flat_model\"].replace(\"MODEL A-MAISONETTE\", \"MODEL A MAISONETTE\")\n",
    "df[\"flat_model\"] = df[\"flat_model\"].replace(\"New Generation\", \"NEW GENERATION\")\n",
    "df[\"flat_model\"] = df[\"flat_model\"].replace(\"Model A2\", \"MODEL A2\")\n",
    "df[\"flat_model\"] = df[\"flat_model\"].replace(\"MULTI GENERATION\", \"MULTI-GENERATION\")\n",
    "df[\"flat_model\"] = df[\"flat_model\"].replace(\"Multi Generation\", \"MULTI-GENERATION\")\n",
    "df[\"flat_model\"] = df[\"flat_model\"].replace(\"Premium Apartment\", \"PREMIUM APARTMENT\")\n",
    "df[\"flat_model\"] = df[\"flat_model\"].replace(\"Premium Apartment Loft\", \"PREMIUM APARTMENT LOFT\")\n",
    "df[\"flat_model\"] = df[\"flat_model\"].replace(\"Premium Maisonette\", \"PREMIUM MAISONETTE\")\n",
    "df[\"flat_model\"] = df[\"flat_model\"].replace(\"Simplified\", \"SIMPLIFIED\")\n",
    "df[\"flat_model\"] = df[\"flat_model\"].replace(\"Standard\", \"STANDARD\")\n",
    "df[\"flat_model\"] = df[\"flat_model\"].replace(\"Terrace\", \"TERRACE\")\n",
    "df[\"flat_model\"] = df[\"flat_model\"].replace(\"Type S1\", \"TYPE S1\")\n",
    "df[\"flat_model\"] = df[\"flat_model\"].replace(\"Type S2\", \"TYPE S2\")\n",
    "\n",
    "df[\"flat_model\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert lease_commence_date to remaining lease at point of transaction\n",
    "# lease commencing 1976 and sale in 1990 =  1976+99-1990 = 85 years remaining\n",
    "# Report Section 4-A\n",
    "df[\"remaining_lease\"] = (df[\"lease_commence_date\"] + 99 - df[\"month\"].str[:4].astype(int))/99\n",
    "df = df.drop(columns=['lease_commence_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load HDB Resale Price Index\n",
    "# Report Section 5-A\n",
    "hdb_resale_price_index = pd.read_csv(\"data/housing-and-development-board-resale-price-index-1q2009-100-monthly.csv\", low_memory=False, index_col=0)\n",
    "current_index = hdb_resale_price_index.tail(1)[\"index\"].values[0]\n",
    "\n",
    "# Calculate adjusted price\n",
    "df = df.join(hdb_resale_price_index, on=\"month\", how=\"left\", rsuffix=\"_index\")\n",
    "df[\"adjusted_price\"] = df[\"resale_price\"] * (current_index / df[\"index\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot median adjusted price vs month\n",
    "# Plot median resale price vs month\n",
    "\n",
    "# Convert month to datetime\n",
    "df[\"month\"] = pd.to_datetime(df[\"month\"], format=\"%Y-%m\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "\n",
    "ax.plot(df.groupby(\"month\")[\"adjusted_price\"].median(), color=\"steelblue\", label=\"Median Adjusted Price\")\n",
    "ax.plot(df.groupby(\"month\")[\"adjusted_price\"].quantile(0.25), color=\"cornflowerblue\", label=\"25 Percentile Adjusted Price\")\n",
    "ax.plot(df.groupby(\"month\")[\"adjusted_price\"].quantile(0.75), color=\"lightsteelblue\", label=\"75 Percentile Adjusted Price\")\n",
    "ax.plot(df.groupby(\"month\")[\"resale_price\"].median(), color=\"firebrick\", label=\"Median Unadjusted Resale Price\")\n",
    "ax.plot(df.groupby(\"month\")[\"resale_price\"].quantile(0.25), color=\"indianred\", label=\"25 Percentile Unadjusted Resale Price\")\n",
    "ax.plot(df.groupby(\"month\")[\"resale_price\"].quantile(0.75), color=\"lightcoral\", label=\"75 Percentile Unadjusted Resale Price\")\n",
    "\n",
    "ax.set_xlabel(\"Time\")\n",
    "ax.set_ylabel(\"Price\")\n",
    "ax.set_title(\"Median Resale Price vs Median Adjusted Price\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=[\"index\", \"resale_price\", \"month\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Region Data\n",
    "# Report Section 5-B\n",
    "# Central = ['BISHAN', 'BUKIT MERAH', 'BUKIT TIMAH', 'CENTRAL AREA', 'GEYLANG', 'KALLANG/WHAMPOA', 'MARINE PARADE', 'QUEENSTOWN', 'TOA PAYOH']\n",
    "# East = ['BEDOK','PASIR RIS', 'TAMPINES']\n",
    "# West = ['BUKIT BATOK', 'BUKIT PANJANG', 'CHOA CHU KANG', 'CLEMENTI', 'JURONG EAST', 'JURONG WEST']\n",
    "# North East = ['ANG MO KIO','HOUGANG', 'PUNGGOL', 'SENGKANG','SERANGOON']\n",
    "# North = ['SEMBAWANG', 'WOODLANDS', 'YISHUN',]\n",
    "\n",
    "def get_region(row):\n",
    "\ttown = row[\"town\"]\n",
    "\tif town in ['BISHAN', 'BUKIT MERAH', 'BUKIT TIMAH', 'CENTRAL AREA', 'GEYLANG', 'KALLANG/WHAMPOA', 'MARINE PARADE', 'QUEENSTOWN', 'TOA PAYOH']:\n",
    "\t\treturn \"CENTRAL\"\n",
    "\telif town in ['BEDOK','PASIR RIS', 'TAMPINES']:\n",
    "\t\treturn \"EAST\"\n",
    "\telif town in ['BUKIT BATOK', 'BUKIT PANJANG', 'CHOA CHU KANG', 'CLEMENTI', 'JURONG EAST', 'JURONG WEST']:\n",
    "\t\treturn \"WEST\"\n",
    "\telif town in ['ANG MO KIO','HOUGANG', 'PUNGGOL', 'SENGKANG','SERANGOON']:\n",
    "\t\treturn \"NORTH-EAST\"\n",
    "\telif town in ['SEMBAWANG', 'WOODLANDS', 'YISHUN']:\n",
    "\t\treturn \"NORTH\"\n",
    "       \n",
    "df[\"region\"] = df.apply(get_region, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add coordinate data from block_street_name_coords.json\n",
    "# Report Section 5-B\n",
    "df[\"block_street_name\"] = df[\"block\"].astype(str) + \" \" + df[\"street_name\"].astype(str)\n",
    "df = df.drop(columns=['block', 'street_name'])\n",
    "block_street_name_coords = pd.read_json(\"data/block_street_name_coords.json\")\n",
    "block_street_name_coords = block_street_name_coords.transpose()\n",
    "\n",
    "df = df.join(block_street_name_coords, on=\"block_street_name\", how=\"left\", rsuffix=\"_coords\")\n",
    "df.drop(columns=[\"block_street_name\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise data on Singapore map\n",
    "import matplotlib.colors as colors\n",
    "def visualise(df, vmin, vmax):\n",
    "    \n",
    "    df_sorted = df.sort_values(by='price_per_sqm')\n",
    "    x = df_sorted['longitude']\n",
    "    y = df_sorted['latitude']\n",
    "    c = df_sorted['price_per_sqm'] \n",
    "\n",
    "    plt.rcParams['figure.figsize'] = [20, 10]\n",
    "    plt.rcParams['figure.dpi'] = 100 \n",
    "\n",
    "    # add image of singapore map\n",
    "    img = plt.imread('data/3247px-Singapore_location_map_(main_island).svg.png')\n",
    "    plt.imshow(img, extent=[103.557, 104.131, 1.129, 1.493])\n",
    "\n",
    "    # set axes limits\n",
    "    plt.xlim(103.62, 104.03)\n",
    "    plt.ylim(1.23, 1.465)\n",
    "    \n",
    "\n",
    "    # Set axes titles\n",
    "    plt.xlabel('Longitude')\n",
    "    plt.ylabel('Latitude')\n",
    "\n",
    "    plt.scatter(x, y, s=0.01, c=c, cmap='OrRd', \n",
    "                norm=colors.Normalize(vmin=vmin,vmax=vmax), alpha=0.8)\n",
    "    cbar = plt.colorbar()\n",
    "    cbar.set_label('Price per sqm', rotation=270, labelpad=20)\n",
    "    ax = plt.gca()\n",
    "    ax.set_aspect('equal', adjustable='box')\n",
    "    plt.show()\n",
    "\n",
    "df2 = df\n",
    "df2[\"price_per_sqm\"] = df2[\"adjusted_price\"] / df2[\"floor_area_sqm\"]\n",
    "visualise(df2, df2[\"price_per_sqm\"].quantile(0.10), df2[\"price_per_sqm\"].quantile(0.90))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add distance from Downtown Core planning area (CBD)\n",
    "# Report Section 5-B\n",
    "# Downtown Core planning area (CBD) = 1.286667, 103.853611\n",
    "dg_mrt_lat = np.radians(1.286667)\n",
    "dg_mrt_long = np.radians(103.853611)\n",
    "\n",
    "df['distance_from_cbd'] = 6367 * 2 * np.arcsin(np.sqrt(np.sin((np.radians(df['latitude']) - dg_mrt_lat)/2)**2 + math.cos(math.radians(37.2175900)) * np.cos(np.radians(df['latitude'])) * np.sin((np.radians(df['longitude']) - dg_mrt_long)/2)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform storey_range to median of range (e.g. 01 TO 03 = 2)\n",
    "# Report Section 4-B-4\n",
    "def convert_to_median(row):\n",
    "\tstorey_range = row[\"storey_range\"].split(\" TO \")\n",
    "\tmedian = (int(storey_range[0]) + int(storey_range[1])) / 2\n",
    "\treturn median\n",
    "\n",
    "df[\"median_storey\"] = df.apply(convert_to_median, axis=1)\n",
    "df = df.drop(columns=['storey_range'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform flat_type to ordinal encoding, town and flat_model to one-hot encoding\n",
    "# Report Section 4-B\n",
    "from sklearn.preprocessing import OrdinalEncoder, StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder \n",
    "from sklearn.compose import ColumnTransformer \n",
    "\n",
    "cols = [\"town\", \"region\", \"longitude\", \"latitude\",\"distance_from_cbd\",\n",
    "         \"flat_type\", \"flat_model\", \"floor_area_sqm\", \"median_storey\", \"remaining_lease\",\n",
    "         \"adjusted_price\"]\n",
    "df = df[cols]\n",
    "\n",
    "pipeline = ColumnTransformer([ \n",
    "# Add normalisation for numerical columns\n",
    "#    (\"s\", StandardScaler(), [\"longitude\", \"latitude\", \"distance_from_cbd\", \"floor_area_sqm\", \"median_storey\", \"remaining_lease\"]),\n",
    "     (\"o\", OrdinalEncoder(), [\"flat_type\"]), \n",
    "     (\"n\", OneHotEncoder(sparse_output=False), [\"town\", \"flat_model\", \"region\"]), \n",
    "     ], \n",
    "     remainder='passthrough', verbose_feature_names_out=False\n",
    ") \n",
    "pipeline.set_output(transform=\"pandas\")\n",
    "df = pipeline.fit_transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description = df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataframe to file\n",
    "import pickle\n",
    "df.to_pickle(\"dataframes/pre-processed.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Test Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"dataframes/pre-processed.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle and split dataset randomly into 80% training and 20% test, then split training into 80% training and 20% validation\n",
    "train, test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "train, val = train_test_split(train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate that the split is correct\n",
    "print(\"Train size: \", len(train))\n",
    "print(\"Validation size: \", len(val))\n",
    "print(\"Test size: \", len(test))\n",
    "\n",
    "# Validate that the split is a representative sample of the original dataset\n",
    "print(\"Train mean: \", train[\"adjusted_price\"].mean())\n",
    "print(\"Validation mean: \", val[\"adjusted_price\"].mean())\n",
    "print(\"Test mean: \", test[\"adjusted_price\"].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into X and y\n",
    "X_train = train.drop(columns=[\"adjusted_price\"])\n",
    "y_train = train[\"adjusted_price\"]\n",
    "\n",
    "X_val = val.drop(columns=[\"adjusted_price\"])\n",
    "y_val = val[\"adjusted_price\"]\n",
    "\n",
    "X_test = test.drop(columns=[\"adjusted_price\"])\n",
    "y_test = test[\"adjusted_price\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_pickle(\"dataframes/x-train.pkl\")\n",
    "y_train.to_pickle(\"dataframes/y-train.pkl\")\n",
    "\n",
    "X_val.to_pickle(\"dataframes/x-val.pkl\")\n",
    "y_val.to_pickle(\"dataframes/y-val.pkl\")\n",
    "\n",
    "X_test.to_pickle(\"dataframes/x-test.pkl\")\n",
    "y_test.to_pickle(\"dataframes/y-test.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load y_train, y_test, X_train, X_test\n",
    "\n",
    "y_train = pd.read_pickle('dataframes/y-train.pkl')\n",
    "y_val = pd.read_pickle('dataframes/y-val.pkl')\n",
    "X_train = pd.read_pickle('dataframes/x-train.pkl')\n",
    "X_val = pd.read_pickle('dataframes/x-val.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get MSE, MAE and MPE for model where we predict the mean of the training set\n",
    "mean = y_train.mean()\n",
    "y_pred = np.full(len(y_val), mean)\n",
    "\n",
    "print(\"MSE: \", mean_squared_error(y_val, y_pred))\n",
    "print(\"MAE: \", mean_absolute_error(y_val, y_pred))\n",
    "print(\"MPE: \", mean_absolute_percentage_error(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate f_regression scores for each feature\n",
    "# from sklearn.feature_selection import f_regression\n",
    "\n",
    "# # Sort feature names by f_regression scores\n",
    "# feature_names = X_train.columns\n",
    "# scores = f_regression(X_train, y_train)[0]\n",
    "# feature_scores = pd.DataFrame({\"feature\": feature_names, \"score\": scores})\n",
    "# feature_scores.sort_values(by=\"score\", ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Linear Regression and evaluate on validation set\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_train, y_train)\n",
    "\n",
    "# Print MSE and MAE\n",
    "y_val_pred = lin_reg.predict(X_val)\n",
    "print(\"MSE for linear regression model 1 =>\", mean_squared_error(y_val, y_val_pred))\n",
    "print(\"MAE for linear regression model 1 =>\", mean_absolute_error(y_val, y_val_pred))\n",
    "print(\"MAPE for linear regression model 1 =>\", mean_absolute_percentage_error(y_val, y_val_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Ridge Regression and evalue on validation set\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "rr = Ridge(alpha=1).fit(X_train, y_train) \n",
    "\n",
    "y_val_pred = rr.predict(X_val)\n",
    "print(\"MSE for ridge regression alpha 1 =>\", mean_squared_error(y_val, y_val_pred))\n",
    "print(\"MAE for ridge regression alpha 1 =>\", mean_absolute_error(y_val, y_val_pred))\n",
    "\n",
    "rr = Ridge(alpha=10).fit(X_train, y_train) \n",
    "\n",
    "y_val_pred = rr.predict(X_val)\n",
    "print(\"MSE for ridge regression alpha 10 =>\", mean_squared_error(y_val, y_val_pred))\n",
    "print(\"MAE for ridge regression alpha 10 =>\", mean_absolute_error(y_val, y_val_pred))\n",
    "\n",
    "rr = Ridge(alpha=100).fit(X_train, y_train) \n",
    "\n",
    "y_val_pred = rr.predict(X_val)\n",
    "print(\"MSE for ridge regression alpha 100 =>\", mean_squared_error(y_val, y_val_pred))\n",
    "print(\"MAE for ridge regression alpha 100 =>\", mean_absolute_error(y_val, y_val_pred))\n",
    "\n",
    "rr = Ridge(alpha=1000).fit(X_train, y_train) \n",
    "\n",
    "y_val_pred = rr.predict(X_val)\n",
    "print(\"MSE for ridge regression alpha 1000 =>\", mean_squared_error(y_val, y_val_pred))\n",
    "print(\"MAE for ridge regression alpha 1000 =>\", mean_absolute_error(y_val, y_val_pred))\n",
    "\n",
    "rr = Ridge(alpha=10000).fit(X_train, y_train) \n",
    "\n",
    "y_val_pred = rr.predict(X_val)\n",
    "print(\"MSE for ridge regression alpha 10000 =>\", mean_squared_error(y_val, y_val_pred))\n",
    "print(\"MAE for ridge regression alpha 10000 =>\", mean_absolute_error(y_val, y_val_pred))\n",
    "\n",
    "rr = Ridge(alpha=100000).fit(X_train, y_train) \n",
    "\n",
    "y_val_pred = rr.predict(X_val)\n",
    "print(\"MSE for ridge regression alpha 10000 =>\", mean_squared_error(y_val, y_val_pred))\n",
    "print(\"MAE for ridge regression alpha 10000 =>\", mean_absolute_error(y_val, y_val_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train support vector regression and evaluate on validation set\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "svr = SVR(kernel=\"linear\", C=1, epsilon=0.1)\n",
    "svr.fit(X_train, y_train)\n",
    "\n",
    "y_val_pred = svr.predict(X_val)\n",
    "print(\"MSE for SVR =>\", mean_squared_error(y_val, y_val_pred))\n",
    "print(\"MAE for SVR =>\", mean_absolute_error(y_val, y_val_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Decison Tree Regressor and evaluate on validation set\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree_reg = DecisionTreeRegressor(random_state=42).fit(X_train, y_train)\n",
    "\n",
    "y_val_pred = tree_reg.predict(X_val)\n",
    "print(\"MSE for decision tree regression model 1 =>\", mean_squared_error(y_val, y_val_pred))\n",
    "print(\"MAE for decision tree regression model 1 =>\", mean_absolute_error(y_val, y_val_pred))\n",
    "print(\"MAPE for decision tree regression model 1 =>\", mean_absolute_percentage_error(y_val, y_val_pred))\n",
    "\n",
    "print(tree_reg.get_depth())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train using k-nearest neighbors\n",
    "# from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "# knn_reg = KNeighborsRegressor(weights=\"distance\", n_jobs=8).fit(X_train, y_train)\n",
    "\n",
    "# y_val_pred = knn_reg.predict(X_val)\n",
    "# print(\"MSE for k-nearst neighbors regression model 1 =>\", mean_squared_error(y_val, y_val_pred))\n",
    "# print(\"MAE for k-nearst neighbors regression model 1 =>\", mean_absolute_error(y_val, y_val_pred))\n",
    "# print(\"MAPE for k-nearst neighbors regression model 1 =>\", mean_absolute_percentage_error(y_val, y_val_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train Linear support vector regressor and evaluate on validation set\n",
    "# from sklearn.svm import LinearSVR\n",
    "\n",
    "# svm_reg = LinearSVR(random_state=42).fit(X_train, y_train)\n",
    "\n",
    "# y_val_pred = svm_reg.predict(X_val)\n",
    "# print(\"MSE for linear support vector regression model 1 =>\", mean_squared_error(y_val, y_val_pred))\n",
    "# print(\"MAE for linear support vector regression model 1 =>\", mean_absolute_error(y_val, y_val_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Random Forest Regressor and evaluate on validation set\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "forest_reg = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=8).fit(X_train, y_train)\n",
    "\n",
    "y_val_pred = forest_reg.predict(X_val)\n",
    "print(\"MSE for random forest regression model 1 =>\", mean_squared_error(y_val, y_val_pred))\n",
    "print(\"MAE for random forest regression model 1 =>\", mean_absolute_error(y_val, y_val_pred))\n",
    "print(\"MAPE for random forest regression model 1 =>\", mean_absolute_percentage_error(y_val, y_val_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth = 0\n",
    "for dt_reg in forest_reg.estimators_:\n",
    "\tif dt_reg.get_depth() > max_depth:\n",
    "\t\tmax_depth = dt_reg.get_depth()\n",
    "\n",
    "print(max_depth)\n",
    "print(len(forest_reg.estimators_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import train and validation data\n",
    "y_train = pd.read_pickle('dataframes/y-train.pkl')\n",
    "y_val = pd.read_pickle('dataframes/y-val.pkl')\n",
    "X_train = pd.read_pickle('dataframes/x-train.pkl')\n",
    "X_val = pd.read_pickle('dataframes/x-val.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of Estimators Hyperparameter Tuning\n",
    "errors = []\n",
    "growing_rf = RandomForestRegressor(n_estimators=1, n_jobs=6,  \n",
    "                                    warm_start=True, random_state=42)\n",
    "for i in [1, 5, 10, 20, 25, 30, 40, 50, 60, 70, 80, 100, 150, 200, 250]:\n",
    "    growing_rf.n_estimators=i\n",
    "    growing_rf.fit(X_train, y_train)\n",
    "    predicted = growing_rf.predict(X_val)\n",
    "    errors.append(mean_squared_error(y_val, predicted))\n",
    "    print('Iteration: ', i, 'RMSE Loss: ', math.sqrt(errors[-1]), ' MAE: ', mean_absolute_error(y_val, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Max Depth Hyperparameter Tuning\n",
    "from sklearn.ensemble import RandomForestRegresso\n",
    "\n",
    "errors = []\n",
    "for i in [1, 5, 10, 15, 20, 25,30, 40, 60, 70]:\n",
    "    growing_rf = RandomForestRegressor(n_estimators=75, n_jobs=6, random_state=42, max_depth=i)\n",
    "    growing_rf.fit(X_train, y_train)\n",
    "    predicted = growing_rf.predict(X_val)\n",
    "    errors.append(mean_squared_error(y_val, predicted))\n",
    "    print('Iteration: ', i, 'RMSE Loss: ', math.sqrt(errors[-1]), ' MAE: ', mean_absolute_error(y_val, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
    "import math, pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import train and validation data\n",
    "y_train = pd.read_pickle('dataframes/y-train.pkl')\n",
    "y_val = pd.read_pickle('dataframes/y-val.pkl')\n",
    "X_train = pd.read_pickle('dataframes/x-train.pkl')\n",
    "X_val = pd.read_pickle('dataframes/x-val.pkl')\n",
    "\n",
    "# Combine train and validation data\n",
    "X_train = pd.concat([X_train, X_val])\n",
    "y_train = pd.concat([y_train, y_val])\n",
    "\n",
    "# Read in test data\n",
    "X_test = pd.read_pickle('dataframes/x-test.pkl')\n",
    "y_test = pd.read_pickle('dataframes/y-test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Random Forest Model with 75 trees and a max depth of 25\n",
    "\n",
    "rf = RandomForestRegressor(n_estimators=75, max_depth=25, random_state=42, n_jobs=6)\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save random forest model to pickle file\n",
    "\n",
    "with open('models/rf.pkl', 'wb') as f:\n",
    "    pickle.dump(rf, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on test data\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "# Print statistics\n",
    "print('Mean Absolute Error:', mean_absolute_error(y_test, y_pred))\n",
    "print('Mean Squared Error:', mean_squared_error(y_test, y_pred))\n",
    "print('Root Mean Squared Error:', math.sqrt(mean_squared_error(y_test, y_pred)))\n",
    "print('Mean Absolute Percentage Error:', mean_absolute_percentage_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot y_pred vs y_test\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.scatter(y_test, y_pred)\n",
    "plt.xlabel('Actual')\n",
    "plt.ylabel('Predicted')\n",
    "plt.title('Actual vs Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histogram of residuals\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.hist(y_test - y_pred, bins=50)\n",
    "plt.xlabel('Residual')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Residual Histogram')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot residuals\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.scatter(y_pred, y_test - y_pred)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Residual')\n",
    "plt.title('Residual vs Predicted')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
